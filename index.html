<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css"> <link rel=stylesheet  href="/css/font-awesome.min.css"> <link rel=stylesheet  href="/css/celeste.min.css"> <link rel=icon  type="image/png" sizes=192x192  href="/assets/favicon.png"> <link rel="shortcut icon" href="/assets/favicon.ico"> <link rel=apple-touch-icon-precomposed  sizes=152x152  href="/assets/apple-touch-icon.png"> <title>Trixi-GPU Home</title> <header> <nav class=nav-main > <ul> <a class=hvr-underline-reveal  href="/" style="margin-top: 15px; margin-right: 15px"> <img src="/assets/favicon.png" alt=Logo  style="height: 45px; width: 45px"> </a> <li class=hvr-underline-reveal ><a href="/">Home</a> <li class=hvr-underline-reveal ><a href="/doc/">Doc</a> <li class=hvr-underline-reveal ><a href="/tutorial/">Tutorial</a> <li class=hvr-underline-reveal ><a href="/blog/">Blog</a> </ul> </nav> </header> <div class=franklin-content ><h1 id=trixi-gpu ><a href="#trixi-gpu" class=header-anchor >Trixi-GPU</a></h1> <div class=franklin-toc ><ol><li><a href="#about_trixi-gpu">About Trixi-GPU</a><li><a href="#trixicudajl">TrixiCUDA.jl</a><ol><li><a href="#recent_update">Recent Update</a></ol><li><a href="#acceleration_overview">Acceleration Overview</a><ol><li><a href="#cpu_and_gpu_data_flow">CPU and GPU Data Flow</a><li><a href="#ode_and_pde_acceleration">ODE and PDE Acceleration</a><li><a href="#smediscretization_on_gpu">Smediscretization on GPU</a></ol><li><a href="#news">News</a><li><a href="#acknowledgments">Acknowledgments</a></ol></div> <h2 id=about_trixi-gpu ><a href="#about_trixi-gpu" class=header-anchor >About Trixi-GPU</a></h2> <p>Trixi-GPU offers GPU acceleration for solving hyperbolic PDEs. The core solvers are based on the <a href="https://trixi-framework.github.io/">Trixi-Framework</a>, with its acceleration powered by <a href="https://juliagpu.org/">JuliaGPU</a>. Currently, NVIDIA CUDA serves as our primary and experimental GPU acceleration support, and other types of GPU support will be implemented in the future.</p> <h2 id=trixicudajl ><a href="#trixicudajl" class=header-anchor >TrixiCUDA.jl</a></h2> <p>TrixiCUDA.jl offers CUDA acceleration for solving hyperbolic PDEs on GPUs. It is our top-priority solution for achieving high acceleration in solving PDEs on GPUs.</p> <h3 id=recent_update ><a href="#recent_update" class=header-anchor >Recent Update</a></h3> <p><em>Update on Dec 31, 2024</em>:</p> <ul> <li><p>The initial round of kernel optimization starts with the volume integral kernels &#40;see <a href="https://github.com/trixi-gpu/TrixiCUDA.jl/pull/102">TrixiCUDA.jl PR #102</a>&#41; and will later extend to all existing kernels used in the semidiscretization process. The primary approach includes improving global memory access patterns, using shared memory, and selecting appropriate launch sizes to reduce warp stalls and achieve better occupancy.</p> </ul> <p><em>Update on Nov 21, 2024</em>: </p> <ul> <li><p>Due to the <a href="https://github.com/trixi-framework/Trixi.jl/issues/2108">issue</a> from upstream with Trixi.jl and CUDA.jl in Julia v1.11, this package now supports only Julia v1.10. Using or developing this package with Julia v1.11 will result in precompilation errors. To fix this, downgrade to Julia v1.10. If you have any other problems, please file issues <a href="https://github.com/trixi-gpu/TrixiCUDA.jl/issues">here</a>.</p> </ul> <p><em>Update on Oct 30, 2024</em>: </p> <ul> <li><p>The general documentation is now available at <a href="https://trixi-gpu.github.io">https://trixi-gpu.github.io</a> &#40;in development&#41;. </p> <li><p>Documentation specific to this package can be found at <a href="https://trixi-gpu.github.io/TrixiCUDA.jl/dev">https://trixi-gpu.github.io/TrixiCUDA.jl/dev</a> &#40;in development&#41;.</p> </ul> <p><a href="/update"><em>Archived Update</em></a></p> <h2 id=acceleration_overview ><a href="#acceleration_overview" class=header-anchor >Acceleration Overview</a></h2> <h3 id=cpu_and_gpu_data_flow ><a href="#cpu_and_gpu_data_flow" class=header-anchor >CPU and GPU Data Flow</a></h3> <div class=row > <div class=container > <img class=left  src="/assets/index1.png"> <div style="clear: both"></div> </div> </div> <p>Minimizing large and frequent data transfers between the CPU and GPU is crucial for accelerating large programs. To reduce transfer overhead, initial values and large parameters are initialized and kept on the GPU throughout the solving process. Additionally, GPU-specific interfaces and custom kernels are implemented to speed up data initialization and processing.</p> <h3 id=ode_and_pde_acceleration ><a href="#ode_and_pde_acceleration" class=header-anchor >ODE and PDE Acceleration</a></h3> <div class=row > <div class=container > <img class=left  src="/assets/index2.png"> <div style="clear: both"></div> </div> </div> <p>The overall GPU acceleration relies on two parts: &#40;1&#41; ODE acceleration, based on GPU array operations natively accelerated through CUDA.jl, and &#40;2&#41; PDE-specific acceleration, focusing on semidiscretization, implemented with custom kernels. With custom kernels, specialized optimizations focused on things like memory access and algorithms can be applied to achieve further speedup.</p> <h3 id=smediscretization_on_gpu ><a href="#smediscretization_on_gpu" class=header-anchor >Smediscretization on GPU</a></h3> <div class=row > <div class=container > <img class=left  src="/assets/index3.png"> <div style="clear: both"></div> </div> </div> <p>Semidiscretization is a key part of acceleration due to its potential for full parallelization and the weak data dependencies between some functionalities. Thus, running it on the GPU with pipelined streams is an effective approach to achieving high speedup. But some data dependencies force certain functionalities to remain sequential, making it hard to achieve more intensive pipelining.</p> <h2 id=news ><a href="#news" class=header-anchor >News</a></h2> <p>There is a <a href="https://julialang.org/jsoc/gsoc/trixi/#adaptive_mesh_refinement_on_gpus_with_cuda_dynamic_parallelism">new project</a> focused on implementing AMR with CUDA dynamic parallelism for the upcoming <a href="https://summerofcode.withgoogle.com/">Google Summer of Code 2025</a>. Please reach out if you are interested.</p> <h2 id=acknowledgments ><a href="#acknowledgments" class=header-anchor >Acknowledgments</a></h2> <p>Thanks to our developers, maintainers, and outside contributors for their contributions to our community. Also, special thanks to Prof. Hendrik Ranocha, Prof. Jesse Chan, and Prof. Michael Schlottke-Lakemper for advising this project.</p> <div class=page-foot > <p> &copy; <a href="https://github.com/trixi-gpu" target=_blank >Trixi-GPU</a> developers. Powered by <a href="https://github.com/tlienart/Franklin.jl" target=_blank >Franklin.jl</a> and the <a href="https://julialang.org" target=_blank >Julia programming language</a>. </p> </div></div>